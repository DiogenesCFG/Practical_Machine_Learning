---
title: "Practical Machine Learning Course Project"
author: "Diogenes Cruz Figueroa Garc√≠a"
date: "26/02/2020"
output:
  html_document:
    df_print: paged
subtitle: Personal activity data
---

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
               echo = FALSE, warning = FALSE, message = FALSE, error = FALSE)
```

```{r loading_libraries}
library(caret); library(dplyr); library(ggplot2); library(kableExtra);
library(randomForest); library(rpart.plot); library(rattle); library(mgcv); 
library(corrplot)

```

# Summary

This document is prepared as a part of the Data Science specialization from the 
John Hopkins University and Coursera, in the Practical Machine Learning course. 
The data set consists of measurements of devices from individuals performing 
weight lifting excercises, either in one correct fashion, or four incorrect 
ones. The objective is to use a training data set and machine learning 
algorithms to predict from these measurements in a testing data set. This 
document serves both for the peer-graded assignment, and for the course project 
prediction quiz.

# Data Overview

The subjects were tracked during weightliftinf excersises, and sensors were 
located in their arms, forearms, and belt areas, and a sensor was also 
positioned in the dumbells. Several three-dimensional measurements were taken 
while the participants did dumbbell biceps curls, in five different fashions 
(classes):

* Classe A: exactly according to the specification.
* Classe B: throwing the elbows to the front.
* Classe C: lifting the dumbbell only halfway.
* Classe D: lowering the dumbbell only halfway.
* Classe E: throwing the hips to the front.

More on the dataset can be read in 
[this link](http://groupware.les.inf.puc-rio.br/har#ixzz3xsbS5bVX).

We'll be working with personal data from smart devices which 
capture a person's movement to track their activity. We'll be working with the 
training data, and testing our results on the testing data, using the `caret` 
package in R. The goal for this project is to predict the manner in which the 
excercise was  made, `classe` variable, using the rest of the variables.

```{r looking}
training <- 
     read.csv(".\\Data\\pml-training.csv", header = TRUE)

testing <- 
     read.csv(".\\Data\\pml-testing.csv", header = TRUE)
```

The training data from our set contains `r dim(training)[1]` observations of 
`r dim(training)[2]` variables. Here's a table on how many variables for 
each class for each user were taken, in our training data set.

```{r fast_table}
t1 <- table(training$user_name, training$classe)
kable(t1, 
      caption = "Users-Classe") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE, 
                latex_options = "hold_position")
```

# Training our data

For this excersise, we will be implementing three algorithm approaches: (1) 
predicting with trees, (2) random forests, and (3) boosting. We will be 
combining our predictors, and compare which one fares better for predicting 
the classe of the weight lifting excersice based on the sensors measurements. 

Keep in mind that we have to prepare our daa, and trim it as much as we can for 
computational reasons.

```{r prep_data, cache = TRUE}
# Remove variables with near zero variance
nzv <- nearZeroVar(training)
training <- training[, -nzv]

# Remove variables with mostly NA
isNA <- sapply(training, function(x) mean(is.na(x))) > 0.95
training <- training[, isNA == FALSE]

# Remove id Variables
training <- training[, -(1:5)]

# Create a partition so to have a validation set
set.seed(950)
inTrain  <- createDataPartition(training$classe, p = 0.7, list = FALSE)

trainSet <- training[inTrain, ]
testSet <- training[-inTrain, ]

dim(trainSet); dim(testSet)
```

We first partitioned our training data into two, so as to have a validation 
set, which is possible due to having a large number of observations. We then 
trim our variables, first by eliminating zero variance predictors using the 
`nearZerovar()` function. We then removed variables with a high proportion of 
`NA` values. Finally, we eliminated the first five variables, which contained 
identification values, with no predictive use. From this data set, we created 
our partition which contained `r dim(trainSet)[2] - 1` predictors.

## Regression Trees

The idea behind predicting with trees is that if you have a bunch of variables, 
you can take each of those variables, and use it to split the outcome into 
different groups. As you split the outcomes into different groups, you can 
evaluate the homogeneity of the outcome within each group, and continue to 
split again if necessary, until you get outcomes that are separated into groups 
that are homogeneous enough, or that they are small enough that you need to 
stop.

We well be using the `rpart` package and the `rpart()` function to train our 
model, and to select trees, we will be using the `rpart` method. The results 
of our model looks as follows:

```{r p_trees, cache = TRUE}
set.seed(135)

ModelTree <- rpart(classe ~ ., data = trainSet, method = "class")
fancyRpartPlot(ModelTree)
```

That's how our fancy model tree looks like. Now let's test our model with our 
testing set partition from the testing data. Remember, we will use the testing 
data until the very end.

```{r testing_trees, cache = TRUE}
# Predicting
predTree <- predict(ModelTree, newdata = testSet, type = "class")
confTree <- confusionMatrix(predTree, testSet$classe)
tree_overall <- as.data.frame(confTree$overall)
names(tree_overall) <- c("Value")

kable(tree_overall, 
      caption = "Overall Statistics") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE, 
                latex_options = "hold_position")

# Plotting decission tree
plot(confTree$table, col = confTree$byClass, 
     main = paste("Decision Tree - Accuracy =",
                  round(confTree$overall["Accuracy"], 4)))

```


## Random Forests

With random forest, the idea is to bootstrap samples, take a resample of 
observed data and training data, and rebuild classification or regression 
trees on each bootstrap sample. For random forests, we need the 
`random forest` package which can be used in conjunction with the `train()` 
function from the `caret` package.

```{r rForest, cache = TRUE}
set.seed(215)
tControl <- trainControl(method="cv", number=3, verboseIter=FALSE)
ModelRF <- train(classe ~ ., data=trainSet, method="rf",
                          trControl = tControl)

# Predicting
predRF <- predict(ModelRF, newdata = testSet)
confRF <- confusionMatrix(predRF, testSet$classe)
RF_overall <- as.data.frame(confRF$overall)
names(RF_overall) <- c("Value")

kable(RF_overall, 
      caption = "Overall Statistics") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE, 
                latex_options = "hold_position")

# Plotting decission tree
plot(confRF$table, col = confRF$byClass, 
     main = paste("Random Forests - Accuracy =",
                  round(confRF$overall["Accuracy"], 4)))

```

## Boosting

The idea behind boosting is taking a bunch of possibly weak predictors, and 
weighting them up so as to potentiate their strengths, and add them up. We 
will be using the `gmb` package along with `caret` to train our model.

```{r boosting, cache = TRUE}
set.seed(122814)
bControl <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
ModelGBM <- train(classe ~ ., data=trainSet, method="gbm",
                          trControl = tControl, verbose = FALSE)

# Predicting
predGBM <- predict(ModelGBM, newdata = testSet)
confGBM <- confusionMatrix(predGBM, testSet$classe)
GBM_overall <- as.data.frame(confGBM$overall)
names(GBM_overall) <- c("Value")

kable(GBM_overall, 
      caption = "Overall Statistics") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE, 
                latex_options = "hold_position")

# Plotting decission tree
plot(confGBM$table, col = confGBM$byClass, 
     main = paste("Boosting - Accuracy =",
                  round(confGBM$overall["Accuracy"], 4)))
```

## Model Comparison

As shown in the previous table, Random Forests fares better than boosting and 
than regression trees, though boosting also has a high accuracy. Let's try 
combining the random forests model with boosting to see if we can kick the 
accuracy up a notch. We'll be using the generalized additive models with 
integrated smoothness estimation.

# Applying selected model

```{r model comparison, cache = TRUE}
Accuracy <- data.frame(Model = 
                            c("Regression Trees", 
                              "Random Forests", 
                              "Boosting"), 
                       Accuracy = 
                            c(round(confTree$overall["Accuracy"], 4), 
                              round(confRF$overall["Accuracy"], 4), 
                              round(confGBM$overall["Accuracy"], 4))
                       )
kable(Accuracy, 
      caption = "Models' accuracy") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE, 
                latex_options = "hold_position")

```

Once we've chosen our model, we'll be using our testing data and predict to 
which class does the weight lifting observation corresponds. According to our 
previous models, random forests had the highest accuracy, so that'll be the 
model we will pick.

So, once we use random forests, we get that the testing data corresponds to 
the following activities

```{r model_testing}
predTesting <- predict(ModelRF, newdata = testing)
table <- cbind(testing$X, as.character(testing$user_name), 
               as.character(predTesting))
colnames(table) <- c("X", "User Name", "Classe Prediction")


kable(table, 
      caption = "Testing data prediction using Random Forests") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE, 
                latex_options = "hold_position")

```

# Apendix - Code Chunks
1. Used libraries
```{r loading_libraries, echo = TRUE, eval = FALSE}
```

2. Reading the data
```{r looking, echo = TRUE, eval = FALSE}
```

3. User - Classe table
```{r fast_table, echo = TRUE, eval = FALSE}
```

4. Preparing the data
```{r prep_data, echo = TRUE, eval = FALSE}
```

5. Regression Trees model
```{r p_trees, echo = TRUE, eval = FALSE}
```

6. Regression Trees predictions
```{r testing_trees, echo = TRUE, eval = FALSE}
```

7. Random Forests model and predictions
```{r rForest, echo = TRUE, eval = FALSE}
```

8. Boosting model and predictions
```{r boosting, echo = TRUE, eval = FALSE}
```

9. Model Comparison table
```{r model comparison, echo = TRUE, eval = FALSE}
```

10. Applying Random Forests to test data
```{r model_testing, echo = TRUE, eval = FALSE}
```